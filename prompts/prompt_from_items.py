from bs4 import BeautifulSoup
from urllib.parse import unquote
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser

from pydantic import BaseModel, Field
from typing import Union, Literal

import pandas as pd

from enrichment.llms import analysis_llm
from files.tables import TableProcessor

# Define the Pydantic Model
class ConditionalAnswer(BaseModel):
    """Response containing an answer or 'none' if the information is not in the context."""
    answer: Union[str, Literal['none']] = Field(
        description="The answer to the question based on the context, or the exact string 'none' if the information is not found."
    )
    
def answer_from_summary(json_fragment, question):
    """
    Queries GPT-4o-mini with a question about a JSON fragment.

    Args:
        json_fragment (str): The JSON fragment as a string.
        question (str): The question to ask about the JSON data.

    Returns:
        str: The answer generated by GPT-4o-mini.
    """

    try:
        # Initialize the ChatOpenAI model, specifying gpt-4o-mini
        llm = analysis_llm

        # Create a prompt template
        prompt = ChatPromptTemplate.from_messages([
            ("system", "You are an expert at extracting information from JSON data. If the context does not contain the information needed to answer the question, the value of the 'answer' field should be the exact string 'none'. Otherwise, provide the answer in the 'answer' field."),
            ("user", "Here is the JSON data, optionally comprising paper titles, abstracts, and lists of authors including their affiliations or biosketches:\n\n{json_fragment}\n\nQuestion: {question}\n\nAnswer:"),
            ("user", "Tip: Respond with a JSON object conforming to the ConditionalAnswer schema.")
        ])

        structured_llm = llm.with_structured_output(ConditionalAnswer)
        # Create a chain
        structured_chain = prompt | structured_llm# | StrOutputParser()

        # Invoke the chain
        response = structured_chain.invoke({"title": json_fragment['title'], "question": question, "json_fragment": json_fragment})

        return response.answer

    except Exception as e:
        return f"An error occurred: {e}"


def summarize_web_page(content: str) -> str:
    """
    Use LangChain to summarize the content of a web page.
    """
    try:
        # Initialize the LLM (e.g., OpenAI GPT)
        llm = analysis_llm

        prompt = ChatPromptTemplate.from_messages([
            ("system", "You are a helpful assistant that summarizes author biographical information from HTML content."),
            ("user", "Summarize the author info from the following HTML:\n\n{html}"),
        ])

        chain = prompt | llm | StrOutputParser()

        summary = chain.invoke({"html": content})
        return summary
    except Exception as e:
        print(f"Error summarizing web page content: {e}")
        return "Summary could not be generated."


def answer_yes_no(question: str):
    """Boolean question to GPT-4o-mini.
    This function is used to ask a yes/no question to the LLM.

    Args:
        question (str): _description_

    Returns:
        str: Yes or No
    """
    llm = analysis_llm
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", "You are a concise assistant. Respond to the following question with strictly 'Yes' or 'No'. No other words or punctuation."),
        ("user", "{question}")
    ])
    
    chain = prompt | llm | StrOutputParser()
    response = chain.invoke({"question": question})
    return response


def summarize_table(df: pd.DataFrame) -> str:
    """
    Use LangChain to summarize the table schema and sampled rows.

    Args:
        df (pd.DataFrame): The DataFrame to summarize.

    Returns:
        str: The summary and description generated by LangChain.
    """
    # Extract schema
    schema = ", ".join([f"{col} ({dtype})" for col, dtype in zip(df.columns, df.dtypes)])

    # Sample rows
    sample_rows = TableProcessor.sample_rows_to_string(df)

    # Combine schema and sample rows into a context
    context = f"Schema: {schema}\n\nSample Rows:\n{sample_rows}"

    # Initialize the GPT model
    llm = analysis_llm

    # Create a prompt template
    prompt = ChatPromptTemplate.from_messages([
        ("system", "You are an expert data analyst. Summarize the table schema and sample rows."),
        ("user", "Here is the table information:\n\n{context}\n\nPlease provide a summary and description.")
    ])

    # Create the LangChain chain
    chain = prompt | llm | StrOutputParser()

    # Run the chain with the context
    response = chain.invoke({"context": context})

    return response
