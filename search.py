##################
## Search
##
## Copyright (C) Zachary G. Ives, 2025
##################

import requests
from graph_db import GraphAccessor

from typing import List
import json

# Initialize the GraphAccessor
graph_db = GraphAccessor()

from bs4 import BeautifulSoup
from urllib.parse import unquote
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.output_parsers import PydanticOutputParser

import tiktoken

graph_accessor = GraphAccessor()


def truncate_text_to_token_limit(text, token_limit=128000):
    """
    Truncates a text string to a specified token limit using tiktoken.

    Args:
        text (str): The text string to truncate.
        token_limit (int): The maximum number of tokens allowed.

    Returns:
        str: The truncated text string.
    """

    encoding = tiktoken.get_encoding("cl100k_base")  # or another appropriate encoding
    tokens = encoding.encode(text)

    if len(tokens) <= token_limit:
        return text  # No truncation needed

    truncated_tokens = tokens[:token_limit]
    truncated_text = encoding.decode(truncated_tokens)
    return truncated_text


def get_line_items_as_str(criteria: List) -> str:
    """
    Converts a list of criteria into a formatted string.

    Args:
        criteria (List): The list of criteria to format.

    Returns:
        str: The formatted string representation of the criteria.
    """
    ret = ""
    for i in range(len(criteria)):
        ret += "- " + criteria[i]['name'] + ': ' + criteria[i]['prompt'] + "\n"
        
    return ret

def get_json_fragment(criteria: List) -> str:
    """
    Converts a list of criteria into a JSON fragment.

    Args:
        criteria (List): The list of criteria to convert.

    Returns:
        str: The JSON fragment representation of the criteria.
    """
    ret = "{"
    for i in range(len(criteria)):
        ret += '"' + criteria[i]['name'] + '": "..."'
        if i > 0:
            ret += ", "
        
    return ret + "}"

def search_over_criteria(question: str, criteria: List) -> str:
    """
    Queries GPT-4o-mini with a question about a JSON fragment.

    Args:
        question (str): The question to ask.
        criteria (List): The list of criteria to consider.

    Returns:
        str: The answer generated by GPT-4o-mini.
    """

    try:
        # Initialize the ChatOpenAI model, specifying gpt-4o-mini
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0) # gpt-4o-preview is the correct model name.

        # Create a prompt template
        prompt = ChatPromptTemplate.from_messages([
            ("system", "You are an expert at taking a question and breaking it into criterion-specific subquestions."),
            ("user", "We are looking for promising sources related to the question. Given answers to the following questions about papers::\n\n{bulleted_list}\n\nFor each of the items, provide a search phrase for an important element of what we are looking for, in the structured JSON fragment below, replacing the '...'. Omit the item is irrelevant. Question: {question}\n\nAnswer:"),
        ])
        
        bulleted_list = get_line_items_as_str(criteria)
        json_fragment = get_json_fragment(criteria)

        # Create a chain
        chain = prompt | llm | StrOutputParser()

        # Invoke the chain
        answer = chain.invoke({"title": 'answers', "question": question, "json_fragment": json_fragment, "bulleted_list": bulleted_list})
        
        if answer.startswith('```'):
            answer = answer.split('```')[1]
            if answer.startswith('json'):
                answer = answer.split('json')[1]
            answer = answer.strip()

        return answer

    except Exception as e:
        print (f"Error processing question '{question}': {e}")
        return f"An error occurred: {e}"

def search_multiple_criteria(criteria: str) -> List:
    try:
        items = json.loads(criteria)
        
        candidates = {}
        
        for criterion in items.keys():
            sub_prompt = items[criterion]
            
            results = graph_accessor.find_related_entity_ids_by_tag(sub_prompt, criterion, 50)
            if results is not None and len(results) > 0:
                candidates[criterion] = results
        print(candidates)
        
        # Intersect the candidates
        intersected_candidates = set()
        for criterion in items.keys():
            if intersected_candidates == set():
                intersected_candidates = set(candidates[criterion])
            else:
                intersected_candidates.intersection_update(set(candidates[criterion]))
            
        # Convert the intersected candidates to a list
        intersected_candidates = list(intersected_candidates)
        # Return the intersected candidates
        if len(intersected_candidates) > 0:
            return intersected_candidates
        else:
            return "No candidates found matching all criteria."
        
    except Exception as e:
        print (f"Error parsing {criteria}: {e}")
        return f"An error occurred parsing {criteria}: {e}"

# def expand_search():
#     """
#     Endpoint to process a user question or prompt using LangChain and GPT-4o-mini
#     with chain-of-thought reasoning.
#     """
#     try:
#         # Get the user question or prompt from the request
#         user_prompt = request.json.get('prompt')
#         if not user_prompt:
#             print ("'prompt' parameter is missing")
#             return jsonify({"error": "'prompt' parameter is required"}), 400

#         # Initialize the GPT-4o-mini model
#         llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

#         # Define the chain-of-thought reasoning prompt
#         prompt = ChatPromptTemplate.from_messages([
#             ("system", "You are a helpful assistant."),
#             ("user", "{question}"),
#             ("assistant", "Let's think about what criteria would be useful in identifying something to read about this topic. Consider the following: (1) topic, (2) what makes authors credible, (3) what would indicate the result is reliable")
#         ])
#         """
#         1. What characteristics should we look for in the authors and sources?
#         2. What field should we focus on for papers?x
#         3. How many citations did the paper receive?
#         4. What are the main topics covered in the paper?
#         5. What are the main findings or conclusions of the paper?
#         6. What are the implications of the findings?
#         7. What are the limitations of the study?
#         8. What future research directions are suggested?
#         9. What are the main contributions of the paper?
#         10. What are the key methodologies used in the paper?
#         11. What are the main theories or frameworks discussed?
#         12. What are the main arguments or claims made by the authors?
#         13. What are the main research questions addressed?
#         14. What are the main hypotheses tested?
#         15. What are the main variables or constructs studied?
#         16. What are the main data sources used?
#         17. What are the main analytical techniques used?
#         18. What are the main results or findings?
#         19. What are the main conclusions drawn?
#         20. What are the main recommendations made?
#         21. What are the main implications for practice or policy?
#         22. What are the main implications for theory or research?
#         23. What are the main implications for future research?
#         """

#         chain = prompt | llm

#         # Run the chain with the user prompt
#         response = chain.invoke({"question": user_prompt})
        
#         print(response.text)

#         # Return the response in JSON format
#         return jsonify({"data": {"message": response.content} }), 200
#     except Exception as e:
#         return jsonify({"error": f"An error occurred: {e}"}), 500

from langchain.llms import OpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain

def generate_rag_answer(paper_titles_and_summaries: List, question: str) -> str:
    """
    Generate a RAG (Retrieval-Augmented Generation) prompt for GPT-4o-mini and return the answer to the question.

    Args:
        paper_titles_and_summaries (List): A list of paper titles and summaries.
        question (str): The question to be answered.

    Returns:
        str: The answer generated by GPT-4o-mini.
    """
    try:
        # Combine paper IDs and summaries into a single context
        context = "\n".join(
            [f"Paper Title: {p['name']}\nSummary: {p['summary']}" for p in paper_titles_and_summaries]
        )

        # Initialize the GPT-4o-mini model
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1)

        # Create a prompt template
        prompt = ChatPromptTemplate.from_messages([
            ("system", "You are an expert assistant that answers questions based on provided research paper summaries."),
            ("user", "Here are the titles and summaries of some research papers:\n\n{context}\n\nQuestion: {question}\n\nAnswer:")
        ])

        # Create a chain
        chain = prompt | llm | StrOutputParser()

        # Run the chain with the context and question
        response = chain.invoke({"context": context, "question": question})

        return response

    except Exception as e:
        print(f"Error generating RAG answer: {e}")
        return f"An error occurred: {e}"